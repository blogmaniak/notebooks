{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Liquor Sales Predictor\n",
    "#### Predicts the sale by product category given future month and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "#!pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/bhan/src/anaconda/anaconda2/envs/kerasJupyter:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "absl-py                   0.2.0                    py27_0  \r\n",
      "astor                     0.6.2                    py27_0  \r\n",
      "backports                 1.0              py27h63c9359_1  \r\n",
      "backports.functools-lru-cache 1.5                       <pip>\r\n",
      "backports.shutil_get_terminal_size 1.0.0            py27h5bc021e_2  \r\n",
      "backports.weakref         1.0.post1        py27h0df1112_0  \r\n",
      "backports_abc             0.5              py27h7b3c97b_0  \r\n",
      "binutils_impl_linux-64    2.28.1               had2808c_3  \r\n",
      "binutils_linux-64         7.2.0               had2808c_27  \r\n",
      "bleach                    1.5.0                    py27_0  \r\n",
      "bokeh                     0.12.16                   <pip>\r\n",
      "ca-certificates           2018.03.07                    0  \r\n",
      "certifi                   2018.4.16                py27_0  \r\n",
      "click                     6.7                       <pip>\r\n",
      "cloudpickle               0.5.3                     <pip>\r\n",
      "configparser              3.5.0            py27h5117587_0  \r\n",
      "cycler                    0.10.0                    <pip>\r\n",
      "dask                      0.17.5                    <pip>\r\n",
      "dbus                      1.13.2               h714fa37_1  \r\n",
      "decorator                 4.3.0                    py27_0  \r\n",
      "distributed               1.21.8                    <pip>\r\n",
      "entrypoints               0.2.3            py27h502b47d_2  \r\n",
      "enum34                    1.1.6            py27h99a27e9_1  \r\n",
      "expat                     2.2.5                he0dffb1_0  \r\n",
      "fontconfig                2.12.6               h49f89f6_0  \r\n",
      "freetype                  2.8                  hab7d2ae_1  \r\n",
      "funcsigs                  1.0.2            py27h83f16ab_0  \r\n",
      "functools32               3.2.3.2          py27h4ead58f_1  \r\n",
      "futures                   3.2.0            py27h7b459c0_0  \r\n",
      "gast                      0.2.0                    py27_0  \r\n",
      "gcc_impl_linux-64         7.2.0                habb00fd_3  \r\n",
      "gcc_linux-64              7.2.0               h550dcbe_27  \r\n",
      "glib                      2.56.1               h000015b_0  \r\n",
      "gmp                       6.1.2                h6c8ec71_1  \r\n",
      "grpcio                    1.11.0           py27hf484d3e_0  \r\n",
      "gst-plugins-base          1.14.0               hbbd80ab_1  \r\n",
      "gstreamer                 1.14.0               hb453b48_1  \r\n",
      "gxx_impl_linux-64         7.2.0                hdf63c60_3  \r\n",
      "gxx_linux-64              7.2.0               h550dcbe_27  \r\n",
      "h5py                      2.7.1            py27h2697762_0  \r\n",
      "hdf5                      1.10.1               h9caa474_1  \r\n",
      "HeapDict                  1.0.0                     <pip>\r\n",
      "html5lib                  0.9999999                py27_0  \r\n",
      "icu                       58.2                 h9c2bf20_1  \r\n",
      "intel-openmp              2018.0.0                      8  \r\n",
      "ipykernel                 4.8.2                    py27_0  \r\n",
      "ipython                   5.7.0                    py27_0  \r\n",
      "ipython_genutils          0.2.0            py27h89fb69b_0  \r\n",
      "ipywidgets                7.2.1                    py27_0  \r\n",
      "jinja2                    2.10             py27h4114e70_0  \r\n",
      "jpeg                      9b                   h024ee3a_2  \r\n",
      "jsonschema                2.6.0            py27h7ed5aa4_0  \r\n",
      "jupyter                   1.0.0                    py27_4  \r\n",
      "jupyter_client            5.2.3                    py27_0  \r\n",
      "jupyter_console           5.2.0            py27hc6bee7e_1  \r\n",
      "jupyter_core              4.4.0            py27h345911c_0  \r\n",
      "keras                     2.1.5                    py27_0  \r\n",
      "kiwisolver                1.0.1                     <pip>\r\n",
      "libedit                   3.1.20170329         h6b74fdf_2  \r\n",
      "libffi                    3.2.1                hd88cf55_4  \r\n",
      "libgcc-ng                 7.2.0                hdf63c60_3  \r\n",
      "libgfortran-ng            7.2.0                hdf63c60_3  \r\n",
      "libgpuarray               0.7.6                h14c3975_0  \r\n",
      "libpng                    1.6.34               hb9fc6fc_0  \r\n",
      "libprotobuf               3.5.2                h6f1eeef_0  \r\n",
      "libsodium                 1.0.16               h1bed415_0  \r\n",
      "libstdcxx-ng              7.2.0                hdf63c60_3  \r\n",
      "libxcb                    1.13                 h1bed415_1  \r\n",
      "libxml2                   2.9.8                hf84eae3_0  \r\n",
      "locket                    0.2.0                     <pip>\r\n",
      "mako                      1.0.7            py27h3d58d4b_0  \r\n",
      "markdown                  2.6.11                   py27_0  \r\n",
      "markupsafe                1.0              py27h97b2822_1  \r\n",
      "matplotlib                2.2.2                     <pip>\r\n",
      "mistune                   0.8.3            py27h14c3975_1  \r\n",
      "mkl                       2018.0.2                      1  \r\n",
      "mkl-service               1.1.2            py27hb2d42c5_4  \r\n",
      "mkl_fft                   1.0.1            py27h3010b51_0  \r\n",
      "mkl_random                1.0.1            py27h629b387_0  \r\n",
      "mock                      2.0.0            py27h0c0c831_0  \r\n",
      "msgpack                   0.5.6                     <pip>\r\n",
      "nbconvert                 5.3.1            py27he041f76_0  \r\n",
      "nbformat                  4.4.0            py27hed7f2b2_0  \r\n",
      "ncurses                   6.1                  hf484d3e_0  \r\n",
      "notebook                  5.5.0                    py27_0  \r\n",
      "numpy                     1.14.2           py27hdbf6ddf_1  \r\n",
      "openssl                   1.0.2o               h20670df_0  \r\n",
      "packaging                 17.1                      <pip>\r\n",
      "pandas                    0.23.0                    <pip>\r\n",
      "pandoc                    1.19.2.1             hea2e7c5_1  \r\n",
      "pandocfilters             1.4.2            py27h428e1e5_1  \r\n",
      "partd                     0.3.8                     <pip>\r\n",
      "pathlib2                  2.3.2                    py27_0  \r\n",
      "pbr                       4.0.2                    py27_0  \r\n",
      "pcre                      8.42                 h439df22_0  \r\n",
      "pexpect                   4.5.0                    py27_0  \r\n",
      "pickleshare               0.7.4            py27h09770e1_0  \r\n",
      "pip                       10.0.1                   py27_0  \r\n",
      "prompt_toolkit            1.0.15           py27h1b593e1_0  \r\n",
      "protobuf                  3.5.2            py27hf484d3e_0  \r\n",
      "psutil                    5.4.6                     <pip>\r\n",
      "ptyprocess                0.5.2            py27h4ccb14c_0  \r\n",
      "pygments                  2.2.0            py27h4a8b6f5_0  \r\n",
      "pygpu                     0.7.6            py27h3010b51_0  \r\n",
      "pyparsing                 2.2.0                     <pip>\r\n",
      "pyqt                      5.9.2            py27h751905a_0  \r\n",
      "python                    2.7.15               h1571d57_0  \r\n",
      "python-dateutil           2.7.3                    py27_0  \r\n",
      "pytz                      2018.4                    <pip>\r\n",
      "pyyaml                    3.12             py27h2d70dd7_1  \r\n",
      "pyzmq                     17.0.0           py27h14c3975_1  \r\n",
      "qt                        5.9.5                h7e424d6_0  \r\n",
      "qtconsole                 4.3.1            py27hc444b0d_0  \r\n",
      "readline                  7.0                  ha6073c6_4  \r\n",
      "scandir                   1.7              py27h14c3975_0  \r\n",
      "scikit-learn              0.19.1           py27h445a80a_0  \r\n",
      "scipy                     1.1.0            py27hfc37229_0  \r\n",
      "send2trash                1.5.0                    py27_0  \r\n",
      "setuptools                39.1.0                   py27_0  \r\n",
      "simplegeneric             0.8.1                    py27_2  \r\n",
      "singledispatch            3.4.0.3          py27h9bcb476_0  \r\n",
      "sip                       4.19.8           py27hf484d3e_0  \r\n",
      "six                       1.11.0           py27h5f960f1_1  \r\n",
      "sortedcontainers          2.0.4                     <pip>\r\n",
      "sqlite                    3.23.1               he433501_0  \r\n",
      "subprocess32              3.5.1                     <pip>\r\n",
      "tblib                     1.3.2                     <pip>\r\n",
      "tensorboard               1.7.0            py27hf484d3e_1  \r\n",
      "tensorflow                1.7.0                         0  \r\n",
      "tensorflow-base           1.7.0            py27hdbcaa40_2  \r\n",
      "termcolor                 1.1.0                    py27_1  \r\n",
      "terminado                 0.8.1                    py27_1  \r\n",
      "testpath                  0.3.1            py27hc38d2c4_0  \r\n",
      "theano                    1.0.1            py27h6bb024c_0  \r\n",
      "tk                        8.6.7                hc745277_3  \r\n",
      "toolz                     0.9.0                     <pip>\r\n",
      "tornado                   5.0.2                    py27_0  \r\n",
      "traitlets                 4.3.2            py27hd6ce930_0  \r\n",
      "wcwidth                   0.1.7            py27h9e3e1ab_0  \r\n",
      "werkzeug                  0.14.1                   py27_0  \r\n",
      "wheel                     0.31.1                   py27_0  \r\n",
      "widgetsnbextension        3.2.1                    py27_0  \r\n",
      "xz                        5.2.4                h14c3975_4  \r\n",
      "yaml                      0.1.7                had09818_2  \r\n",
      "zeromq                    4.2.5                h439df22_0  \r\n",
      "zict                      0.1.3                     <pip>\r\n",
      "zlib                      1.2.11               ha838bed_2  \r\n"
     ]
    }
   ],
   "source": [
    "#!pip install \"dask[complete]\"\n",
    "#!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset file\n",
    "import sys\n",
    "import types\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#IBM DSX specific file I/O client libraries\n",
    "#from botocore.client import Config\n",
    "#import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "###########---- ibm dsx specific code to load file ----------###########################\n",
    "## Loads the large 13MM record file\n",
    "## The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "## You might want to remove those credentials before you share your notebook.\n",
    "#client_e0d982f9f7984059b4c6b128d1814552 = ibm_boto3.client(service_name='s3',\n",
    "#ibm_api_key_id='PkAbaFpfA7qkLJYNcF1OasbQWapxuH6P-vycQqiofFvK',\n",
    "#ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "#config=Config(signature_version='oauth'),\n",
    "#endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "#body = client_e0d982f9f7984059b4c6b128d1814552.get_object(Bucket='cnn-donotdelete-pr-4iz30eoowmkw91',Key='Iowa_Liquor_Sales.csv')['Body']\n",
    "## add missing __iter__ method, so pandas accepts body as file-like object\n",
    "#if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "##insert credentials for file - Change to credentials_1\n",
    "## @hidden_cell\n",
    "## The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "## You might want to remove those credentials before you share your notebook.\n",
    "#credentials_1 = {\n",
    "#    'IBM_API_KEY_ID': 'PkAbaFpfA7qkLJYNcF1OasbQWapxuH6P-vycQqiofFvK',\n",
    "#    'IAM_SERVICE_ID': 'iam-ServiceId-a42a0a9d-b15f-482d-8ad4-7f333af35771',\n",
    "#    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "#    'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "#    'BUCKET': 'cnn-donotdelete-pr-4iz30eoowmkw91',\n",
    "#    'FILE': 'liqSales.csv'\n",
    "#}\n",
    "## @hidden_cell\n",
    "## The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "## You might want to remove those credentials before you share your notebook.\n",
    "## Make sure this uses the variable above. The number will vary in the inserted code.\n",
    "#try:\n",
    "#    credentials = credentials_1\n",
    "#except NameError as e:\n",
    "#    print('Error: Setup is incorrect or incomplete.\\n')\n",
    "#    print('Follow the instructions to insert the file credentials above, and edit to')\n",
    "#    print('make the generated credentials_# variable match the variable used here.')\n",
    "#    raise\n",
    "#\n",
    "## reference using String\n",
    "#cols = ['Date', 'Category Name', 'Item Description', 'Sale (Dollars)', 'Volume Sold (Liters)']\n",
    "#df = pd.read_csv(body, usecols=cols)\n",
    "###########---- ibm dsx specific code ----------###########################\n",
    "\n",
    "##--- local methods to load file w/ 13MM records ---#\n",
    "#fileLoc = \"../data/Iowa_Liquor_Sales.csv\"\n",
    "#dfs = pd.read_csv(fileLoc)\n",
    "##--- local methods to load file ---#\n",
    "    \n",
    " \n",
    "    \n",
    "#df.head()\n",
    "#count rows and slice into smaller chunk\n",
    "#df['Date'].count()\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_short = df.iloc[0:1000000]\n",
    "#df_short.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Fields to simplify\n",
    "#df_short.rename(columns={'Sale (Dollars)': 'sales'}, inplace=True)\n",
    "#df_short.rename(columns={'Volume Sold (Liters)': 'volume'}, inplace=True)\n",
    "#df_short.rename(columns={'Category Name': 'categoryName'}, inplace=True)\n",
    "#df_short.rename(columns={'Date': 'date'}, inplace=True)\n",
    "#df_short.rename(columns={'Item Description': 'item'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the df Date text to datetime field\n",
    "#import dateutil\n",
    "#df_short['date'] = df_short['date'].apply(dateutil.parser.parse, dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The largest sale possible\n",
    "#df_short['sales'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The largest sale possible\n",
    "#df_short['volume'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print df_short detailss\n",
    "#df_short.index\n",
    "#df_short['date'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save to csv file at object storage\n",
    "#cos = ibm_boto3.client(service_name='s3',\n",
    "#    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "#    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "#    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "#    config=Config(signature_version='oauth'),\n",
    "#    endpoint_url=credentials['ENDPOINT'])\n",
    "#\n",
    "## Build the enriched file name from the original filename.\n",
    "#localfilename = 'enriched_' + credentials['FILE']\n",
    "\n",
    "# Write a CSV file from the enriched pandas DataFrame.\n",
    "#df_short.to_csv(localfilename, index=False)\n",
    "\n",
    "# Use the above put_file method with credentials to put the file in Object Storage.\n",
    "#cos.upload_file(localfilename, Bucket=credentials['BUCKET'],Key=localfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -alt \"../work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable DASK based dataframes\n",
    "# Loading the shortened file into panda dataframe\n",
    "fileLoc = \"../data/enriched_liqSales.csv\"\n",
    "#dfs = dd.read_csv(fileLoc, blocksize=10e6, engine=\"python\").compute()\n",
    "dfs = pd.read_csv(fileLoc, engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs.index\n",
    "#list(dfs)\n",
    "#dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Fields to simplify using the df\n",
    "dfs = dfs.rename(columns={'Date': 'date', 'Store Name': 'storeName', \\\n",
    "                    'Zip Code': 'zip', \\\n",
    "                    'Store Location': 'storeLocation', \\\n",
    "                    'Category Name': 'categoryName', \\\n",
    "                    'Vendor Name': 'vendorName', \\\n",
    "                    'Item Description': 'item', \\\n",
    "                    'Bottle Volume (ml)': 'bottleVol', \\\n",
    "                    'State Bottle Cost': 'bottleCost', \\\n",
    "                    'State Bottle Retail': 'bottleSalePrice', \\\n",
    "                    'Sale (Dollars)': 'txAmount', \\\n",
    "                    'Bottles Sold': 'txNumBottleSold', \\\n",
    "                    'Volume Sold (Liters)': 'txVolLtrs', \\\n",
    "                    'Volume Sold (Gallons)': 'txVolGal'\n",
    "                   })\n",
    "\n",
    "#dfs.rename(columns={'Date': 'date'}, inplace=True)\n",
    "#dfs.rename(columns={'Store Name': 'storeName'}, inplace=True)\n",
    "#dfs.rename(columns={'Zip Code': 'zip'}, inplace=True)\n",
    "#dfs.rename(columns={'Store Location': 'storeLocation'}, inplace=True)\n",
    "#dfs.rename(columns={'Category Name': 'categoryName'}, inplace=True)\n",
    "#dfs.rename(columns={'Vendor Name': 'vendorName'}, inplace=True)\n",
    "#dfs.rename(columns={'Item Description': 'item'}, inplace=True)\n",
    "#dfs.rename(columns={'Bottle Volume (ml)': 'bottleVol'}, inplace=True)\n",
    "#dfs.rename(columns={'State Bottle Cost': 'bottleCost'}, inplace=True)\n",
    "#dfs.rename(columns={'State Bottle Retail': 'bottleSalePrice'}, inplace=True)\n",
    "#dfs.rename(columns={'Sale (Dollars)': 'txAmount'}, inplace=True)\n",
    "#dfs.rename(columns={'Bottles Sold': 'txNumBottleSold'}, inplace=True)\n",
    "#dfs.rename(columns={'Volume Sold (Liters)': 'txVolLtrs'}, inplace=True)\n",
    "#dfs.rename(columns={'Volume Sold (Gallons)': 'txVolGal'}, inplace=True)\n",
    "#list(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert panda to dask df to see if transformation is any faster\n",
    "#dfs.head()\n",
    "dfs = dd.from_pandas(dfs, npartitions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs.divisions\n",
    "#dfs.head()\n",
    "#dfs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currently Sales is a text, convert it to float\n",
    "#Also convert all other numerics to float for supporting future transactions\n",
    "dfs['txAmount'] = dfs['txAmount'].str.replace('$', '')\n",
    "dfs['txAmount'] = dfs['txAmount'].astype(float)\n",
    "dfs['Pack'] = dfs['Pack'].astype(float)\n",
    "dfs['bottleVol'] = dfs['bottleVol'].astype(float)\n",
    "dfs['bottleCost'] = dfs['bottleCost'].str.replace('$', '')\n",
    "dfs['bottleCost'] = dfs['bottleCost'].astype(float)\n",
    "dfs['bottleSalePrice'] = dfs['bottleSalePrice'].str.replace('$', '')\n",
    "dfs['bottleSalePrice'] = dfs['bottleSalePrice'].astype(float)\n",
    "dfs['txNumBottleSold'] = dfs['txNumBottleSold'].astype(float)\n",
    "dfs['txVolLtrs'] = dfs['txVolLtrs'].astype(float)\n",
    "\n",
    "#df_short.head()\n",
    "\n",
    "#Convert date field to panda date/time\n",
    "#dfs['date'] = pd.to_datetime(dfs['date'])\n",
    "dfs['date'] = dfs['date'].map(lambda x: pd.to_datetime(x, errors='coerce'))\n",
    "#dfs['date'] = pd.to_datetime('datetime64[ns]')\n",
    "\n",
    "#Insert columns to indicate Month\n",
    "dfs['month'] = dfs['date'].dt.month\n",
    "\n",
    "#Insert columns to indicate dayofweek\n",
    "dfs['dayofweek'] = dfs['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column assignment doesn't support type DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-43bee8669a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#(?<=\\().*?(?=\\)) looks for everything between () in 1013 MAIN\\nKEOKUK 52632\\n(40.39978, -91.387531)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Then it breaks into individual lat/long using , delimiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latlong'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'storeLocation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'((?<=\\().*?(?=\\)))'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'long'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latlong'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#dfs['lat']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhan/src/anaconda/anaconda2/envs/kerasJupyter/lib/python2.7/site-packages/dask/dataframe/core.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2380\u001b[0m                                 for k, c in zip(key, value.columns)})\n\u001b[1;32m   2381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhan/src/anaconda/anaconda2/envs/kerasJupyter/lib/python2.7/site-packages/dask/dataframe/core.pyc\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2554\u001b[0m                     callable(v) or pd.api.types.is_scalar(v)):\n\u001b[1;32m   2555\u001b[0m                 raise TypeError(\"Column assignment doesn't support type \"\n\u001b[0;32m-> 2556\u001b[0;31m                                 \"{0}\".format(type(v).__name__))\n\u001b[0m\u001b[1;32m   2557\u001b[0m         \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Column assignment doesn't support type DataFrame"
     ]
    }
   ],
   "source": [
    "#Extract the latitude and longitude from storeLocation\n",
    "#(?<=\\().*?(?=\\)) looks for everything between () in 1013 MAIN\\nKEOKUK 52632\\n(40.39978, -91.387531)\n",
    "#Then it breaks into individual lat/long using , delimiter\n",
    "dfs['latlong'] = dfs['storeLocation'].str.extract('((?<=\\().*?(?=\\)))', expand=True)\n",
    "dfs[['lat','long']]  = dfs['latlong'].str.split(',', expand=True)\n",
    "#dfs['lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the profit per day, based on (txAmount - (bottleCost*txNumBottleSold))\n",
    "dfs['dailyProfit'] = dfs['txAmount'] - (dfs['bottleCost']*dfs['txNumBottleSold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs['txAmount']\n",
    "#dfs.head()\n",
    "#list(dfs)\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df for byItem, byLat, byLong all numeric columns\n",
    "dfsModel = dfs[['date','storeName','item','txAmount','month','dayofweek','latlong','lat','long','dailyProfit']]\n",
    "dfsModel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debut to explore data, where a given store has multiple locations but lat/long\n",
    "#import numpy as np\n",
    "#list(dfsModel)\n",
    "#dfsUnq = dfsModel.groupby(\"storeName\").agg({\"txAmount\": np.sum, \"latlong\": pd.Series.nunique})\n",
    "#dfsUnq = dfsUnq.sort_values('storeName')\n",
    "#dfsUnq.head(1000)\n",
    "\n",
    "#show dfs by storeName\n",
    "#dfs.loc[dfs['storeName'] == 'Liquor and Tobacco Outlet /']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To-Do --- visualize early dfs\n",
    "#Show profit graph over date/time\n",
    "#from bokeh.io import output_notebook, show\n",
    "#from bokeh.plotting import figure\n",
    "#output_notebook()\n",
    "\n",
    "#profitShow = TimeSeries(dfs, x=date, y=[dailyProfit], legend=True, plot_width=900, plot_height=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generate sales histogram by distribution\n",
    "#import numpy as np\n",
    "#from numpy import argmax\n",
    "#from bokeh.plotting import figure\n",
    "#from bokeh.io import show, output_notebook\n",
    "#\n",
    "## Create a blank figure with labels\n",
    "#p = figure(plot_width = 600, plot_height = 600, \n",
    "#           title = 'Sales Amount Distribution',\n",
    "#           x_axis_label = 'X', y_axis_label = 'Y')\n",
    "#\n",
    "##Build the histogram with bin range between $0-$150\n",
    "##Bin size of 15 dollars\n",
    "#arr_hist, edges = np.histogram(dfs['txAmount'], \n",
    "#                               bins = int(1200/50), \n",
    "#                               range = [0, 1200])\n",
    "## Put the information in a dataframe\n",
    "#dfSale = pd.DataFrame({'NumTxs': arr_hist, \n",
    "#                       'left': edges[:-1], \n",
    "#                       'right': edges[1:]})\n",
    "#dfSale\n",
    "##arr_hist, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the blank plot\n",
    "#p = figure(plot_height = 600, plot_width = 600, \n",
    "#           title = 'Distribution of Sales',\n",
    "#          x_axis_label = 'Transaction Amount', \n",
    "#           y_axis_label = 'Number Of Transactions') \n",
    "#\n",
    "## Add a quad glyph\n",
    "#p.quad(bottom=0, top=dfSale['NumTxs'], \n",
    "#       left=dfSale['left'], right=dfSale['right'], \n",
    "#       fill_color='#E83151', line_color='#DBD4D3')\n",
    "#\n",
    "## Show the plot\n",
    "#show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing encoder\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#encoder = LabelEncoder()\n",
    "#dfs['categoryNameCode'] = encoder.fit_transform(dfs['categoryName'])\n",
    "##encoder.fit_transform(dfs.loc[:, ['categoryName', 'item']])\n",
    "#dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform df to aggregate sales, volume by item for given date\n",
    "#dfTItem = dfs.drop('categoryName', axis=1)\n",
    "#dfTItem = dfTItem.groupby([\"date\", 'item'], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the final encoded, transformed, shifted and inferred ready df\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "#lets build dfTransform, class that allows to fit and transform\n",
    "#selected label columns\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "    \n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = encoder.fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = encoder.fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "\n",
    "#Fit and Transform the label columns\n",
    "#dfTItem = MultiColumnLabelEncoder(columns = ['item']).fit_transform(dfTItem)\n",
    "\n",
    "#Generate OneHotEncoded for all category sets\n",
    "dfOHE = pd.get_dummies(dfsModel[['storeName','item']], prefix=['sN', 'itm'])\n",
    "#Concat the main dfTItem with OHE df\n",
    "dfsModel = pd.concat([dfsModel, dfOHE], axis=1)\n",
    "#dfTransform[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsModel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert first example\n",
    "#dfOHE\n",
    "#argmax(dfOHE.iloc[3, :])\n",
    "#inverted = encoder.inverse_transform([argmax(dfOHE.iloc[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all unique classes across both labels\n",
    "#list(encoder.classes_)\n",
    "#len(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add shifted Exponentially Weighted Windows to dataframe\n",
    "#dfTransform.loc[:, ['salesMean']]\n",
    "#ema = dfTransform.loc[:, ['salesMean']].ewm(com=0.5, min_periods=2, axis=1).mean()\n",
    "#ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "values = dfs.values\n",
    "# specify columns to plot\n",
    "groups = [3,4]\n",
    "i = 1\n",
    "# plot each column\n",
    "pyplot.figure()\n",
    "for group in groups:\n",
    "\tpyplot.subplot(len(groups), 1, i)\n",
    "\tpyplot.plot(values[:, group])\n",
    "\tpyplot.title(dfs.columns[group], y=0.5, loc='right')\n",
    "\ti += 1\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the LSTM model\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    " \n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "print(reframed.head())\n",
    " \n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_hours = 365 * 24\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    " \n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    " \n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
